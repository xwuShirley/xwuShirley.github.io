<!doctype html>
<html class="no-js" lang="">
<head>
  <meta charset="utf-8">
  <title>Xiaoxia Wu</title>
  <meta name="description" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="">
  <meta property="og:type" content="">
  <meta property="og:url" content="">
  <meta property="og:image" content="">

  <link rel="manifest" href="site.webmanifest">
  <link rel="apple-touch-icon" href="icon.png">
  <!-- Place favicon.ico in the root directory -->

  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/main.css">
  <link rel="stylesheet" href="css/jemdoc.css">

  <meta name="theme-color" content="#fafafa">
</head>

<body>
<div id="layout-content">
  <div id="toptitle">
    <h1>Xiaoxia Wu</h1>
  </div>
  <table class="imgtable">
    <tbody>
    <tr>
      <td>
        <img src="img/self.jpg" alt="None"/>
        &nbsp;
      </td>
      <td align="left">
        <p>
          <a href="#">Xiaoxia (Shirley) Wu (吴晓霞)</a>
          <br>
          Researcher
          <br>
          Microsoft
        </p>
        <p>Email: my first name and my last name AT microsoft dot com</p>
        <p>
          <a href="https://scholar.google.com/citations?user=Ry0Bdt8AAAAJ&hl=en">Google Scholar</a>
        </p>
      </td>
    </tr>
    </tbody>
  </table>
  <h2>About me</h2>
  <p>
    I am currently a researcher at Microsoft, where I work on exciting and cutting-edge methods to reduce the time/budget in large-scale neural networks training.  More information, please check <a href="https://github.com/microsoft/DeepSpeed">deepspeed.ai</a>.
    My research interests are in the areas of large-scale optimization and, more broadly, machine learning.   My Ph.D. study is about efficient and robust methods (to hyperparameter tuning) such as adaptive gradient descent and batch normalization. I am always interested in chatting about research opportunities and collaboration.
  </p>
  <p>
    I was a postdoctoral research fellow mentored by <a href="https://voices.uchicago.edu/willett/">Rebecca Willett</a> at <a href="https://stat.uchicago.edu/">University of Chicago</a> and  <a href="https://ttic.edu/"> Toyota Techonological Institute at Chicago</a>. I have successfully completed the Ph.D. program at The University of Texas at Austin, where I was fortunately
    advised by <a href="https://sites.google.com/prod/view/rward">Rachel Ward</a> and informally co-advised by <a
    href="https://leon.bottou.org/">Léon
    Bottou</a>. I was a research intern at Facebook
    AI Research
    (New York office) during Fall 2017, and a research intern at Google working with <a
    href="https://scholar.google.com/citations?user=LWeVRdUAAAAJ&hl=en">Ethan Dyer</a> and
    <a href="https://www.neyshabur.net/">Behnam Neyshabur</a>
    during Summer 2020.
  </p>
  <p>
    I hold an M.Sc. with Distinction at the University of Edinburgh in Financial Mathematics. Before that, I spent a
    wonderful four-year in the Department of Mathematics and Applied Mathematics at Shantou University where I was
    awarded <a href="https://lksf.org/">Li-Kashing Scholarship</a> to participate in <a
    href="https://www.semesteratsea.org/">Semester at Sea</a>. I
    am from
    Guangdong, China,
    speaking Cantonese and
    Hakka.
  </p>
  <h2>Papers and Preprints (updated on Nov 2021) </h2>
    <ul>
    <li>
      <p>
        <a href="https://scholar.google.com/citations?hl=en&user=Ry0Bdt8AAAAJ&view_op=list_works&sortby=pubdate"> Google Scholar</a>
      </p>
      <b>See my recent papers</b>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/2110.07435">Adaptive Differentially Private Empirical Risk Minimization</a>
        <br>
        Xiaoxia Wu, Lingxiao Wang, Irina Cristali, Quanquan Gu, Rebecca Willett
        <br>
        <b>arXiv:2110.07435</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/2109.08282">AdaLoss: A computationally-efficient and provably convergent adaptive gradient method</a>
        <br>
        Xiaoxia Wu, Yuege Xie, Simon Du and Rachel Ward
        <br>
        <b>arXiv:2109.08282</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/2104.07545">Hierarchical Learning for Generation with Long Source Sequences</a>
        <br>
        Tobias Rohde, Xiaoxia Wu, and Yinhan Liu
        <br>
        <b>arXiv:2104.07545 </b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://openreview.net/forum?id=tW4QEInpni">When Do Curricula Work?</a>
        <br>
        Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur
        <br>
        <b>ICLR (Oral, 53 papers accepted as oral out of 2997 submissions), 2021</b>
        <br>
        [<a href="https://github.com/google-research/understanding-curricula">code</a>, <a
        href="https://drive.google.com/file/d/1T0CyHFQIMA-FSybqD-sK-VtdmT_5Xacj/view">slides</a>]
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/1911.07956">Implicit Regularization and Convergence for Weight
          Normalization</a>
        <br>
        Xiaoxia Wu*, Edgar Dobriban*, Tongzheng Ren*, Shanshan Wu*, Yuanzhi Li, Suriya Gunasekar, Rachel Ward and Qiang
        Liu
        <br>
        <b>NeurIPS, 2020</b>
        <br>
        [<a href="https://drive.google.com/file/d/1uxJ5pZutcV-pkT0_tfwD98DCkv7xULtr/view">slides</a>]
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/2001.03316">Choosing the Sample with Lowest Loss makes SGD Robust</a>
        <br>
        Vatsal Shah, Xiaoxia Wu, and Sujay Sanghavi
        <br>
        <b>AISTATS, 2020</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/1908.10525">Linear Convergence of Adaptive Stochastic Gradient Descent</a>
        <br>
        Yuege Xie, Xiaoxia Wu, and Rachel Ward
        <br>
        <b>AISTATS, 2020</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/1902.07111">Global Convergence of Adaptive Gradient Methods for An
          Over-parameterized Neural Network</a>
        <br>
        Xiaoxia Wu, Simon S. Du, and Rachel Ward
        <br>
        <b>preprint, 2019</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://www.jmlr.org/papers/volume21/18-352/18-352.pdf">AdaGrad stepsizes: Sharp convergence over
          nonconvex landscapes</a>
        <br>
        Rachel Ward*, Xiaoxia Wu*, Leon Bottou
        <br>
        <b>ICML (Oral), 2019</b>
        <br>
        (The longer version is published in <b>Journal of Machine Learning Research</b>)
        <br>
        [<a href="https://github.com/xwuShirley/pytorch/blob/master/torch/optim/adagradnorm.py">code</a>, <a
        href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4691">20 mins video and slides</a>, <a
        href="https://www.jiqizhixin.com/articles/2019-06-15-3">机器之心报导</a>]
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        <a href="https://arxiv.org/abs/1803.02865">WNGrad: Learn the Learning Rate in Gradient Descent</a>
        <br>
        Xiaoxia Wu*, Rachel Ward*, Leon Bottou
        <br>
        <b>preprint, 2018 </b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        An Optimal Mortgage Refinancing Strategy with Stochastic Interest Rate
        <br>
        Xiaoxia Wu, Dejun Xie, David A Edwards
        <br>
        <b>Computational Economics, 1-23, 2018</b>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        Value-at-Risk estimation with stochastic interest rate models for option-bond portfolios
        <br>
        Xiaoyu Wang, Dejun Xie, Jingjing Jiang, Xiaoxia Wu, Jia He
        <br>
        <b>Finance Research Letters 21 (2017): 10-20</b>
      </p>
    </li>
  </ul>

  <br>
  *: indicating equal contribution.
  <!--  <ul style="list-style-type:none">-->
  <!--  </ul>-->

  <h2>Teaching Assistant at UT Austin</h2>
  <ul>
    <li>
      <p>
        Probability I, <i>Spring 19</i>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        Sci Computation in Numerical Analysis, <i>Spring 18</i>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        Linear Algebra and Matrix Theory, <i>Spring 17</i>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        Seq, Series, and Multivariate Calculus, <i>Spring16, Fall16</i>
      </p>
    </li>
  </ul>

  <ul>
    <li>
      <p>
        Differential and Integral Calculus, <i>Fall 14, Spring 15, Fall 16</i>
      </p>
    </li>
  </ul>

</div>
<!-- Add your site or application content here -->

<script src="js/vendor/modernizr-3.11.2.min.js"></script>
<script src="js/plugins.js"></script>
<script src="js/main.js"></script>

<!-- Google Analytics: change UA-XXXXX-Y to be your site's ID. -->
<script>
  window.ga = function () {
    ga.q.push(arguments)
  };
  ga.q = [];
  ga.l = +new Date;
  ga('create', 'UA-XXXXX-Y', 'auto');
  ga('set', 'anonymizeIp', true);
  ga('set', 'transport', 'beacon');
  ga('send', 'pageview')
</script>
<script src="https://www.google-analytics.com/analytics.js" async></script>
</body>

</html>
